标题,标题链接,search-detail,limit_width1,https,作者,日期,https1,down,详情标题,日期时间,内容
人工智能的基本原理,https://blog.csdn.net/qq_41889087/article/details/81119043,最近公司没项目，就看自己了解下人工智能，看了一些资料，觉得https://blog.csdn.net/jiangjunshow/article/details/77711593写的挺好懂的，自己又总结了下：

 

怎么实现人工智能：

实现人工智能的方法有很多种，当前最热门的一种就是通过深度学习来训练神经网络

    

什么是神经网络：

     根据https://blo...,https://blog.csdn.net/qq_41889087/article/details/81119043,//blog.csdn.net/qq_41889087/article/details/81119043,,,,,人工智能的基本原理 - qq_41889087的博客,2018年07月19日 17:18:07,原        人工智能的基本原理
      
      
        
                                                  2018年07月19日 17:18:07
          泪過無痕
          阅读数：3344
                  
        
                  
      
    
  
  
    
            
                              
          
                    最近公司没项目，就看自己了解下人工智能，看了一些资料，觉得https://blog.csdn.net/jiangjunshow/article/details/77711593写的挺好懂的，自己又总结了下：

 

怎么实现人工智能：

实现人工智能的方法有很多种，当前最热门的一种就是通过深度学习来训练神经网络

    

什么是神经网络：

     根据https://blog.csdn.net/jiangjunshow/article/details/77368314所说有点像灰色系统，也有点像线性回归，通过一对对xy（x+中间算法-->y）值来完善中间算法，使其输入x值后，结果越来越接近y值

 

什么是监督学习型神经网络：

有xy值的就是监督学习型神经网络，而只有x值没有y值的就是无监督学习，也称聚类算法

 

什么是深度学习火起来：

    深度学习主要依赖数据、算法、计算力。随着互联网的发展，数据信息变得更加庞大，电脑的普及，也使得计算力越来越强，以及一些算法的优化，是的算法越来越成熟

 

如何将待遇测数据输入到神经系统中：

以一张64*64像素的照片为例，如下图，想要判断图片有没有猫，结果只有两个，有或没有，由于任何颜色都可以由红色、 绿色、蓝色构成，所以图一可以看成是3张64*64维数的颜色表组成（一个像素就是一个颜色点，一个颜色点由红绿蓝三个值来表示，例如红绿蓝分别是255,255,255，那么这个颜色点就是白色），则x就可以看成一个n*1或者1*n维数的向量，n=64*64*3

 



神经网络系统是如何进行预测的：

    逻辑回归（logistics regression）





W1表示x1的权重，b可以理解为一个阀值

 

σ，它代表了sigmoid函数, sigmoid函数的作用就是把计算结果转换为0和1之间值（因为我们的结果只有两个，有或没有，也就是1或0，因此我们的结果要控制在1根0之间，越接近1表示预测越准确）





 

神经网络如何判断自己预测得是否准确：

损失函数（loss function），原理就是通过比较预测的值跟实际的值相差大不大，有此来判断算是否准确

理论上我们可以用（上面的i角标指代某一个训练样本）



但实际上我们不用以上公式，而用下面的公式



 

两个都是结果越小表示预测精度越高

 

     成本函数：针对整个训练集的损失函数，其实就是算整个训练集损失函数的平均值



      结果越大，说明成本越大，预测越不准确

 

神经网络是如何进行学习的：

首先，其实学习就是想办法让损失函数的值变小，而损失函数值是由w跟b决定的，所以问题可以转换成如何寻找合适的w跟b值





损失函数J的形状是一个漏斗，我们训练的目的就是找到漏斗底部的一组w和b。这种形状的函数我们成为凸函数（向下凸起的函数），这也是我们选择J为损失函数的原因，而我们之前遇到的平方差不是一个凸函数，很难找到最小值



如上图所示，梯度下降算法会一步一步地更新w和b，使损失函数一步一步地变得更小，最终找到最小值或接近最小值的地方。



那么到底这个神秘的梯度下降算法是如何来更新w和b的呢？为了简化问题，让大家更容易理解其中的理论，我们先假设损失函数J只有一个参数w（实际上J是一个关于w和b的函数），并且假设w只是一个实数（实际上w是一个向量/一组实数）。如上图，梯度下降算法一步一步地在改变着w的值，在使损失函数的结果越来越小（将w的值一步一步的移到红点处）。我们是通过下面的公式来改变w的值的。

w’ = w – r * dw

其中dw为偏导数，也可以理解为斜率，r为学习率，用来控制w改变的步径，要是选择不当的话可能就如上图所示，直接跳过红点到绿色三角形那里，也就是说你的神经网络可能永远都找不到损失函数的最小值
